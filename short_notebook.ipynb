{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from collections import defaultdict\n",
    "from typing import List\n",
    "from conllu import parse_incr, TokenList\n",
    "from torch import Tensor\n",
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "CUTOFF = 2000\n",
    "from lstm.model import RNNModel\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready\n",
      "Tokenizer ready\n"
     ]
    }
   ],
   "source": [
    "transformer = GPT2Model.from_pretrained('distilgpt2', output_hidden_states=True)\n",
    "print(\"Model ready\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "print(\"Tokenizer ready\")\n",
    "# Note that some models don't return the hidden states by default.\n",
    "# This can be configured by passing `output_hidden_states=True` to the `from_pretrained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Gulordava LSTM model can be found here: \n",
    "# https://drive.google.com/open?id=1w47WsZcZzPyBKDn83cMNd0Hb336e-_Sy\n",
    "#\n",
    "# N.B: I have altered the RNNModel code to only output the hidden states that you are interested in.\n",
    "# If you want to do more experiments with this model you could have a look at the original code here:\n",
    "# https://github.com/facebookresearch/colorlessgreenRNNs/blob/master/src/language_models/model.py\n",
    "#\n",
    "model_location = 'lstm/gulordava.pt'\n",
    "lstm = RNNModel('LSTM', 50001, 650, 650, 2)\n",
    "lstm.load_state_dict(torch.load(model_location))\n",
    "\n",
    "\n",
    "# This LSTM does not use a Tokenizer like the Transformers, but a Vocab dictionary that maps a token to an id.\n",
    "with open('lstm/vocab.txt') as f:\n",
    "    w2i = {w.strip(): i for i, w in enumerate(f)}\n",
    "\n",
    "vocab = defaultdict(lambda: w2i[\"<unk>\"])\n",
    "vocab.update(w2i)\n",
    "i2w = { w2i[k]:k for k in w2i}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time 0.09005069732666016\n"
     ]
    }
   ],
   "source": [
    "from utils import create_or_load_pos_data\n",
    "from controltasks import save_or_load_pos_controls \n",
    "from datasets import find_distribution, POSDataset\n",
    "import torch.utils.data as data \n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "train_x, train_y, vocab, words_train = create_or_load_pos_data(\"train\", transformer, tokenizer, cutoff=CUTOFF)\n",
    "end = time.time() - start\n",
    "print(\"Time %s\" % end )\n",
    "dev_x, dev_y, vocab, words_dev = create_or_load_pos_data(\"dev\", transformer, tokenizer, vocab, cutoff=CUTOFF)\n",
    "test_x, test_y, vocab, words_test = create_or_load_pos_data(\"test\", transformer, tokenizer, vocab, cutoff=CUTOFF)\n",
    "dist = find_distribution(data.DataLoader(POSDataset(train_x, train_y), batch_size=1))\n",
    "\n",
    "flatten_train = [word for sublist in words_train for word in sublist]\n",
    "flatten_dev   = [word for sublist in words_dev for word in sublist]\n",
    "flatten_test  = [word for sublist in words_test for word in sublist]\n",
    "\n",
    "ypos_train_control, ypos_dev_control, ypos_test_control = save_or_load_pos_controls(\n",
    "    train_x, train_y, [flatten_train, flatten_dev, flatten_test], dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching for 2000\n",
      "Doing LSTM: False\n",
      "Data created,pickling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/.local/lib/python3.8/site-packages/torch/storage.py:34: FutureWarning: pickle support for Storage will be removed in 1.5. Use `torch.save` instead\n",
      "  warnings.warn(\"pickle support for Storage will be removed in 1.5. Use `torch.save` instead\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching for 2000\n",
      "Doing LSTM: False\n",
      "Data created,pickling\n",
      "Fetching for 2000\n",
      "Doing LSTM: False\n",
      "Data created,pickling\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "from tree_utils import create_or_load_structural_data\n",
    "from controltasks import save_or_load_struct_controls\n",
    "\n",
    "train_xy = create_or_load_structural_data(\"train\", transformer, tokenizer, cutoff=CUTOFF)\n",
    "dev_xy = create_or_load_structural_data(\"dev\", transformer, tokenizer, cutoff=CUTOFF)\n",
    "test_xy = create_or_load_structural_data(\"test\", transformer, tokenizer, cutoff=CUTOFF)\n",
    "print(len(train_xy))\n",
    "struct_train_control, struct_dev_control, struct_test_control = save_or_load_struct_controls(cutoff=CUTOFF)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PoS Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "import torch.nn as nn\n",
    "class POSProbe(nn.Module):\n",
    "    def __init__(self, repr_size, pos_size):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(repr_size, pos_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "def eval_given_dataloader(loader, model):\n",
    "    model.eval()\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for x,y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        outputs = model(x)\n",
    "        preds = torch.argmax(outputs,dim=1)\n",
    "        c = torch.sum(torch.eq(preds, y))\n",
    "        correct += c.item()\n",
    "        total += y.shape[0]\n",
    "    return correct/total\n",
    "    \n",
    "def train(my_model, train_loader, dev_loader, epoch_amount = 10):\n",
    "    ce = nn.CrossEntropyLoss()\n",
    "    optim = torch.optim.Adam(my_model.parameters())\n",
    "    for i in range(epoch_amount):\n",
    "        my_model.train()\n",
    "        epoch_correct = 0.0\n",
    "        epoch_total = 0.0\n",
    "        for x,y in train_loader:\n",
    "            \n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            outputs = my_model(x)\n",
    "            preds = torch.argmax(outputs,dim=1)\n",
    "            correct = torch.sum(torch.eq(preds, y))\n",
    "            accuracy = correct.item()/y.shape[0]\n",
    "            loss = ce(outputs, y)\n",
    "\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "            \n",
    "            epoch_correct += correct.item()\n",
    "            epoch_total += y.shape[0]\n",
    "        print(\"Epoch\",i,\"accuracy\", epoch_correct/epoch_total, eval_given_dataloader(dev_loader, my_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal task\n",
    "ntrain_loader = data.DataLoader(POSDataset(train_x, train_y), batch_size=16)\n",
    "ndev_loader = data.DataLoader(POSDataset(dev_x, dev_y), batch_size=16)\n",
    "ntest_loader = data.DataLoader(POSDataset(test_x, test_y), batch_size=16)\n",
    "\n",
    "model = POSProbe(768, len(dist)).to(device)\n",
    "train(model, ntrain_loader, ndev_loader, 10)\n",
    "print(\"Test accuracy\", eval_given_dataloader(ntest_loader, model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normal task\n",
    "ctrain_loader = data.DataLoader(POSDataset(train_x, ypos_train_control), batch_size=16)\n",
    "cdev_loader = data.DataLoader(POSDataset(dev_x, ypos_dev_control), batch_size=16)\n",
    "ctest_loader = data.DataLoader(POSDataset(test_x, ypos_test_control), batch_size=16)\n",
    "print(len(ypos_train_control), len(train_x))\n",
    "print(len(ypos_dev_control), len(dev_x))\n",
    "print(len(ypos_test_control), len(test_x))\n",
    "\n",
    "model = POSProbe(768, len(dist)).to(device)\n",
    "train(model, ctrain_loader, cdev_loader, 10)\n",
    "print(\"Test accuracy\", eval_given_dataloader(ctest_loader, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class StructuralProbe(nn.Module):\n",
    "    \"\"\" Computes squared L2 distance after projection by a matrix.\n",
    "    For a batch of sentences, computes all n^2 pairs of distances\n",
    "    for each sentence in the batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dim, rank, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.probe_rank = rank\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        self.proj = nn.Parameter(data = torch.zeros(self.model_dim, self.probe_rank))\n",
    "        \n",
    "        nn.init.uniform_(self.proj, -0.05, 0.05)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\" Computes all n^2 pairs of distances after projection\n",
    "        for each sentence in a batch.\n",
    "        Note that due to padding, some distances will be non-zero for pads.\n",
    "        Computes (B(h_i-h_j))^T(B(h_i-h_j)) for all i,j\n",
    "        Args:\n",
    "          batch: a batch of word representations of the shape\n",
    "            (batch_size, max_seq_len, representation_dim)\n",
    "        Returns:\n",
    "          A tensor of distances of shape (batch_size, max_seq_len, max_seq_len)\n",
    "        \"\"\"\n",
    "        transformed = torch.matmul(batch, self.proj)\n",
    "        \n",
    "        batchlen, seqlen, rank = transformed.size()\n",
    "        \n",
    "        transformed = transformed.unsqueeze(2)\n",
    "        transformed = transformed.expand(-1, -1, seqlen, -1)\n",
    "        transposed = transformed.transpose(1,2)\n",
    "        \n",
    "        diffs = transformed - transposed\n",
    "        \n",
    "        squared_diffs = diffs.pow(2)\n",
    "        squared_distances = torch.sum(squared_diffs, -1)\n",
    "\n",
    "        return squared_distances\n",
    "\n",
    "    \n",
    "class L1DistanceLoss(nn.Module):\n",
    "    \"\"\"Custom L1 loss for distance matrices.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, label_batch, length_batch):\n",
    "        \"\"\" Computes L1 loss on distance matrices.\n",
    "        Ignores all entries where label_batch=-1\n",
    "        Normalizes first within sentences (by dividing by the square of the sentence length)\n",
    "        and then across the batch.\n",
    "        Args:\n",
    "          predictions: A pytorch batch of predicted distances\n",
    "          label_batch: A pytorch batch of true distances\n",
    "          length_batch: A pytorch batch of sentence lengths\n",
    "        Returns:\n",
    "          A tuple of:\n",
    "            batch_loss: average loss in the batch\n",
    "            total_sents: number of sentences in the batch\n",
    "        \"\"\"\n",
    "        labels_1s = (label_batch != -1).float()\n",
    "        predictions_masked = predictions * labels_1s\n",
    "        labels_masked = label_batch * labels_1s\n",
    "        total_sents = torch.sum((length_batch != 0)).float()\n",
    "        squared_lengths = length_batch.pow(2).float()\n",
    "\n",
    "        if total_sents > 0:\n",
    "            loss_per_sent = torch.sum(torch.abs(predictions_masked - labels_masked), dim=(1,2))\n",
    "            normalized_loss_per_sent = loss_per_sent / squared_lengths\n",
    "            batch_loss = torch.sum(normalized_loss_per_sent) / total_sents\n",
    "        \n",
    "        else:\n",
    "            batch_loss = torch.tensor(0.0)\n",
    "        \n",
    "        return batch_loss, total_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "import math\n",
    "import tree_utils\n",
    "import importlib\n",
    "importlib.reload(tree_utils)\n",
    "\n",
    "# I recommend you to write a method that can evaluate the UUAS & loss score for the dev (& test) corpus.\n",
    "# Feel free to alter the signature of this method.\n",
    "def evaluate_probe(probe, dataloader):\n",
    "    loss_function =  L1DistanceLoss()\n",
    "    probe.eval()\n",
    "    total_loss = 0.0\n",
    "    total_uuas = 0.0\n",
    "    amt = 0.0\n",
    "    for distances, embs, lengths in dataloader:\n",
    "        embs = embs.to(device)\n",
    "        distances = distances.to(device)\n",
    "        lengths = lengths.to(device)\n",
    "        amt += len(distances)\n",
    "        outputs = probe(embs)\n",
    "        loss = loss_function(outputs, distances, lengths)[0]\n",
    "        total_loss += loss.item()\n",
    "        for i in range(len(distances)):\n",
    "            l = lengths[i]\n",
    "            preds = outputs[i,0:l, 0:l]\n",
    "            gold = distances[i,0:l, 0:l]\n",
    "            \n",
    "            u = tree_utils.calc_uuas(preds, gold)\n",
    "            if math.isnan(u):\n",
    "                amt -= 1\n",
    "            # This if statement is a hack so nans don't get counted\n",
    "            if u >= 0: total_uuas += u\n",
    "    \n",
    "    return total_loss/amt, total_uuas/amt\n",
    "\n",
    "# Feel free to alter the signature of this method.\n",
    "def train_structural(probe, dataloader, dev_dataloader,test_loader, epochs=100):\n",
    "    lr = 1e-5\n",
    "    batch_size = 128\n",
    "    \n",
    "    optimizer = optim.Adam(probe.parameters(), lr=lr)\n",
    "    #scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,patience=1)\n",
    "    loss_function =  L1DistanceLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        probe.train()\n",
    "        for distances, embs, lengths in dataloader:\n",
    "            embs = embs.to(device)\n",
    "            distances = distances.to(device)\n",
    "            lengths = lengths.to(device)\n",
    "            outputs = probe(embs)\n",
    "            loss = loss_function(outputs, distances, lengths)[0]\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        dev_loss, dev_uuas = evaluate_probe(probe, dev_dataloader)\n",
    "        print(\"Epoch\", epoch, \"Dev loss and uuas\", dev_loss, dev_uuas)\n",
    "        # Using a scheduler is up to you, and might require some hyper param fine-tuning\n",
    "        #scheduler.step(dev_loss)\n",
    "\n",
    "    test_loss, test_uuas = evaluate_probe(probe, test_loader)\n",
    "    print(\"Test loss, uuas\", test_loss, test_uuas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructuralProbe()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anna/Documents/uni/nlp2/nlp2-probing-lms/tree_utils.py:120: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  uuas = np.sum([pred_edge in gold_edges for pred_edge in pred_edges]) / len(gold_edges)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Dev loss and uuas 4.322278783697831 0.2554878169040452\n",
      "Epoch 1 Dev loss and uuas 3.995628646047492 0.25578798663549884\n",
      "Epoch 2 Dev loss and uuas 3.6987896326968546 0.25512165235793105\n",
      "Epoch 3 Dev loss and uuas 3.429826969347502 0.2553972539621388\n",
      "Epoch 4 Dev loss and uuas 3.168339699193051 0.2566106704089192\n",
      "Epoch 5 Dev loss and uuas 2.9386144136127674 0.2564556272393384\n",
      "Epoch 6 Dev loss and uuas 2.7277887806139494 0.2570196120596311\n",
      "Epoch 7 Dev loss and uuas 2.530672193828382 0.2576877365284407\n",
      "Epoch 8 Dev loss and uuas 2.3508224788464998 0.2576064262261719\n",
      "Epoch 9 Dev loss and uuas 2.1809371888010127 0.2579926265895921\n",
      "Epoch 10 Dev loss and uuas 2.0264882398906505 0.25747813217573556\n",
      "Epoch 11 Dev loss and uuas 1.8880232519852487 0.25860887347670963\n",
      "Epoch 12 Dev loss and uuas 1.7560892908196701 0.2589392123268599\n",
      "Epoch 13 Dev loss and uuas 1.642160379510177 0.2586569617161382\n",
      "Epoch 14 Dev loss and uuas 1.5221234432019686 0.2594767737507225\n",
      "Epoch 15 Dev loss and uuas 1.4176638573094418 0.2609491522411292\n",
      "Epoch 16 Dev loss and uuas 1.3242135098105983 0.26080714435914026\n",
      "Epoch 17 Dev loss and uuas 1.2362698324103105 0.2615962174083769\n",
      "Epoch 18 Dev loss and uuas 1.153653454027678 0.2626244340244222\n",
      "Epoch 19 Dev loss and uuas 1.0788345266643324 0.2624651731599259\n",
      "Epoch 20 Dev loss and uuas 1.008195807808324 0.2649407167191537\n",
      "Epoch 21 Dev loss and uuas 0.945531688489412 0.2660765971112702\n",
      "Epoch 22 Dev loss and uuas 0.8885498267725894 0.2672015984215543\n",
      "Epoch 23 Dev loss and uuas 0.833063860441509 0.26804231634398257\n",
      "Epoch 24 Dev loss and uuas 0.7818509623878881 0.26951111108542325\n",
      "Test loss, uuas 0.7940583907313576 0.2761398589495943\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from datasets import StructuralDataset, pad_batch\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = data.DataLoader(StructuralDataset(*train_xy), batch_size=batch_size, collate_fn= pad_batch, shuffle=True)\n",
    "dev_loader = data.DataLoader(StructuralDataset(*dev_xy), batch_size=batch_size, collate_fn= pad_batch, shuffle=True)\n",
    "test_loader = data.DataLoader(StructuralDataset(*test_xy), batch_size=batch_size, collate_fn= pad_batch, shuffle=True)\n",
    "\n",
    "emb_dim = 768\n",
    "rank = 64\n",
    "probe = StructuralProbe(emb_dim, rank).to(device)\n",
    "print(probe)\n",
    "train_structural(probe, train_loader, dev_loader, test_loader, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructuralProbe()\n",
      "Epoch 0 Dev loss and uuas 4.107690293161492 0.26768809081198003\n",
      "Epoch 1 Dev loss and uuas 3.8062079359355727 0.26781423561827383\n",
      "Epoch 2 Dev loss and uuas 3.5304531659578022 0.26824722242866683\n",
      "Epoch 3 Dev loss and uuas 3.277516118099815 0.26889425368342434\n",
      "Epoch 4 Dev loss and uuas 3.0451397303531045 0.2692873369640357\n",
      "Epoch 5 Dev loss and uuas 2.8313979700991982 0.2687838029042891\n",
      "Epoch 6 Dev loss and uuas 2.634600693552118 0.2682790186142832\n",
      "Epoch 7 Dev loss and uuas 2.453245415938528 0.2678841727768955\n",
      "Epoch 8 Dev loss and uuas 2.285989004436292 0.2684587114568794\n",
      "Epoch 9 Dev loss and uuas 2.131622334530479 0.26874505800212883\n",
      "Epoch 10 Dev loss and uuas 1.9890561254400956 0.26969611653548425\n",
      "Epoch 11 Dev loss and uuas 1.8573055949964021 0.2703686675804443\n",
      "Epoch 12 Dev loss and uuas 1.735480341660349 0.27183069372828284\n",
      "Epoch 13 Dev loss and uuas 1.6227739434493216 0.27239191889971776\n",
      "Epoch 14 Dev loss and uuas 1.5184553066052888 0.2727452519103894\n",
      "Epoch 15 Dev loss and uuas 1.4218615411457263 0.273286022368498\n",
      "Epoch 16 Dev loss and uuas 1.332389403895328 0.2741392076403885\n",
      "Epoch 17 Dev loss and uuas 1.2494896898771588 0.2735896681233791\n",
      "Epoch 18 Dev loss and uuas 1.1726605214570698 0.273048104940811\n",
      "Epoch 19 Dev loss and uuas 1.101442763679906 0.272881474319951\n",
      "Epoch 20 Dev loss and uuas 1.03541503253736 0.27223903875657995\n",
      "Epoch 21 Dev loss and uuas 0.9741899696149324 0.2731265502477735\n",
      "Epoch 22 Dev loss and uuas 0.9174101357710989 0.2738978650018379\n",
      "Epoch 23 Dev loss and uuas 0.8647456716236315 0.2749641255774712\n",
      "Epoch 24 Dev loss and uuas 0.8158915354076185 0.27520433105290043\n",
      "Test loss, uuas 0.8293286576794572 0.2847109119570864\n"
     ]
    }
   ],
   "source": [
    "from datasets import StructuralDataset, pad_batch\n",
    "\n",
    "batch_size = 32\n",
    "ctrain_loader = data.DataLoader(StructuralDataset(struct_train_control, train_xy[1]), batch_size=batch_size, collate_fn= pad_batch)\n",
    "cdev_loader = data.DataLoader(StructuralDataset(struct_dev_control, dev_xy[1]), batch_size=batch_size, collate_fn= pad_batch)\n",
    "ctest_loader = data.DataLoader(StructuralDataset(struct_test_control, test_xy[1]), batch_size=batch_size, collate_fn= pad_batch)\n",
    "\n",
    "emb_dim = 768\n",
    "rank = 64\n",
    "probe = StructuralProbe(emb_dim, rank).to(device)\n",
    "print(probe)\n",
    "train_structural(probe, ctrain_loader, cdev_loader, ctest_loader, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tree_utils' from '/home/anna/Documents/uni/nlp2/nlp2-probing-lms/tree_utils.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(tree_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tree_utils import *\n",
    "struct_train_control[1].shape\n",
    "#print(train_xy[1][0].shape)\n",
    "t = edges(create_mst(train_xy[0][1]))\n",
    "print_tikz(t, edges(create_mst(struct_train_control[1])), [\"w\"]*18, 'test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
