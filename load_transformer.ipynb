{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Torch and GPT2 model\n",
    "import torch\n",
    "from torch.distributions import Categorical\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "\n",
    "# logging\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "# Widgets\n",
    "from IPython.display import display\n",
    "from ipywidgets import widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore some irrelevant warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:transformers.configuration_utils:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at /home/albert/.cache/torch/transformers/4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.4c1d7fc2ac6ddabeaf0c8bec2ffc7dc112f668f5871a06efcff113d2797ec7d5\n",
      "INFO:transformers.configuration_utils:Model config GPT2Config {\n",
      "  \"_num_labels\": 2,\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"GPT2LMHeadModel\"\n",
      "  ],\n",
      "  \"attn_pdrop\": 0.1,\n",
      "  \"bad_words_ids\": null,\n",
      "  \"bos_token_id\": 50256,\n",
      "  \"decoder_start_token_id\": null,\n",
      "  \"do_sample\": false,\n",
      "  \"early_stopping\": false,\n",
      "  \"embd_pdrop\": 0.1,\n",
      "  \"eos_token_id\": 50256,\n",
      "  \"finetuning_task\": null,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"is_decoder\": false,\n",
      "  \"is_encoder_decoder\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"length_penalty\": 1.0,\n",
      "  \"max_length\": 20,\n",
      "  \"min_length\": 0,\n",
      "  \"model_type\": \"gpt2\",\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"no_repeat_ngram_size\": 0,\n",
      "  \"num_beams\": 1,\n",
      "  \"num_return_sequences\": 1,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": null,\n",
      "  \"prefix\": null,\n",
      "  \"pruned_heads\": {},\n",
      "  \"repetition_penalty\": 1.0,\n",
      "  \"resid_pdrop\": 0.1,\n",
      "  \"summary_activation\": null,\n",
      "  \"summary_first_dropout\": 0.1,\n",
      "  \"summary_proj_to_labels\": true,\n",
      "  \"summary_type\": \"cls_index\",\n",
      "  \"summary_use_proj\": true,\n",
      "  \"task_specific_params\": null,\n",
      "  \"temperature\": 1.0,\n",
      "  \"top_k\": 50,\n",
      "  \"top_p\": 1.0,\n",
      "  \"torchscript\": false,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n",
      "INFO:transformers.modeling_utils:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at /home/albert/.cache/torch/transformers/4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "INFO:transformers.modeling_utils:Weights of GPT2LMHeadModel not initialized from pretrained model: ['lm_head.weight']\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at /home/albert/.cache/torch/transformers/f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:transformers.tokenization_utils:loading file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at /home/albert/.cache/torch/transformers/d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "# load the model and set in eval mode \n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval();\n",
    "model.to('cuda');\n",
    "\n",
    "# initialize tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_words(model, text, paragraph_len, temp = None):\n",
    "    \"\"\"\n",
    "    Function that predicts next words given an input sentence\n",
    "    \n",
    "    Args:\n",
    "      Model: The transformer model to be used\n",
    "      Text : The input text\n",
    "      paragraph_len: Amount of words to be added to original sequence\n",
    "      Temp: Temperature used in sampling instead of argmax\n",
    "    Returns:\n",
    "      Prints the predicted sequence\n",
    "    \"\"\"\n",
    "    \n",
    "    # Encode the tokens and get em cudatorch-y\n",
    "    indexed_tokens = tokenizer.encode(text)\n",
    "    tokens_tensor = torch.tensor([indexed_tokens])\n",
    "    tokens_tensor = tokens_tensor.to('cuda')\n",
    "    \n",
    "    # Predict stuff\n",
    "    with torch.no_grad():\n",
    "        for i in range(paragraph_len):\n",
    "            \n",
    "            # Pass through model\n",
    "            outputs = model(tokens_tensor)\n",
    "            predictions = outputs[0]\n",
    "            \n",
    "            if temp == None:\n",
    "                # Get argmax\n",
    "                predicted_index = torch.argmax(predictions[0,-1,:]).item()\n",
    "            else:\n",
    "                # Sample with temperature\n",
    "                epsilon = 1e-8\n",
    "                x = predictions[0,-1,:] * temp\n",
    "                m = Categorical(logits = (x + epsilon))\n",
    "                predicted_index = m.sample()\n",
    "                #print(sum(m.probs))\n",
    "                #print(sum(m.logits))\n",
    "            \n",
    "            # Add prediction to previous predictions\n",
    "            predicted_index = torch.tensor([predicted_index]).to('cuda').unsqueeze(0)\n",
    "            tokens_tensor = torch.cat((tokens_tensor, predicted_index), dim=1)\n",
    "    \n",
    "    # Squeeze and decode\n",
    "    tokens_tensor = tokens_tensor.squeeze(0)\n",
    "    predicted_text = tokenizer.decode(tokens_tensor.tolist())\n",
    "\n",
    "    print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13e9d3bcf63461999450ec44c5c8e8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Text(value='', placeholder='Type something!')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c72aab480244009ba887c5e7af813e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Submit!', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: unspecified launch failure",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-48d42fada262>\u001b[0m in \u001b[0;36mbutton_click\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mglobal\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputText\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mpredict_words\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparagraph_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Textbox stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-d40692dd90cb>\u001b[0m in \u001b[0;36mpredict_words\u001b[0;34m(model, text, paragraph_len, temp)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mindexed_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mtokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindexed_tokens\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mtokens_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens_tensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Predict stuff\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: unspecified launch failure"
     ]
    }
   ],
   "source": [
    "# Initialize input widget and get standard text\n",
    "inputText = widgets.Text(placeholder='Type something!')\n",
    "text = \"Who was Jim Henson ? Jim Henson was a\"\n",
    "\n",
    "# How many words we predict\n",
    "paragraph_len = 40\n",
    "temp = 3\n",
    "\n",
    "# Text event handler\n",
    "def getText(s):\n",
    "    global text\n",
    "    text = inputText.value\n",
    "    \n",
    "# Button eventhandler\n",
    "def button_click(b):\n",
    "    global text, model\n",
    "    text = inputText.value\n",
    "    predict_words(model, text, paragraph_len, temp)\n",
    "    \n",
    "# Textbox stuff\n",
    "inputText.on_submit(getText)\n",
    "display(inputText)\n",
    "inputText\n",
    "    \n",
    "# Button stuff\n",
    "button = widgets.Button(description=\"Submit!\")\n",
    "display(button)\n",
    "button.on_click(button_click)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(numbers):\n",
    "    total = sum(np.exp(numbers))\n",
    "    \n",
    "    return [np.exp(num)/total for num in numbers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.09003057317038046, 0.24472847105479767, 0.6652409557748219]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[0.09003057317038046, 0.24472847105479767, 0.6652409557748219]\n",
      "[0.2534976524899556, 0.29590914376846866, 0.45059320374157574]\n",
      "[0.30663708811686846, 0.319921744103127, 0.3734411677800045]\n",
      "[0.32441628763919456, 0.3287548004361928, 0.34682891192461257]\n",
      "[0.33035860693641345, 0.33179498560077175, 0.3378464074628149]\n",
      "[0.33234148068347963, 0.3328191919005808, 0.3348393274159394]\n",
      "[0.3330026845016226, 0.33316180162232456, 0.3338355138760528]\n",
      "[0.3332231135624029, 0.33327613928332245, 0.33350074715427475]\n",
      "[0.3332965930208367, 0.33331426678153947, 0.33338914019762383]\n"
     ]
    }
   ],
   "source": [
    "result = np.array([1,2,3])\n",
    "for i in range(10):\n",
    "    print(result)\n",
    "    result = softmax(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
