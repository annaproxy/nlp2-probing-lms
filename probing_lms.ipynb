{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Probing Language Models__\n",
    "\n",
    "This notebook serves as a start for your NLP2 assignment on probing Language Models. This notebook will become part of the contents that you will submit at the end, so make sure to keep your code (somewhat) clean :-)\n",
    "\n",
    "__note__: This is the first time _anyone_ is doing this assignment. That's exciting! But it might well be the case that certain aspects are too unclear. Do not hesitate at all to reach to me once you get stuck, I'd be grateful to help you out.\n",
    "\n",
    "__note 2__: This assignment is not dependent on big fancy GPUs. I run all this stuff on my own 3 year old CPU, without any Colab hassle. So it's up to you to decide how you want to run it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models\n",
    "\n",
    "For the Transformer models you are advised to make use of the `transformers` library of Huggingface: https://github.com/huggingface/transformers\n",
    "Their library is well documented, and they provide great tools to easily load in pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports ready\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Model, GPT2Tokenizer\n",
    "import torch\n",
    "print(\"Imports ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model ready\n",
      "Tokenizer ready\n"
     ]
    }
   ],
   "source": [
    "model = GPT2Model.from_pretrained('distilgpt2', output_hidden_states=True)\n",
    "print(\"Model ready\")\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n",
    "print(\"Tokenizer ready\")\n",
    "# Note that some models don't return the hidden states by default.\n",
    "# This can be configured by passing `output_hidden_states=True` to the `from_pretrained` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-f46bbdc6b9ce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mpredicted_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpredicted_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Voc size\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "# Predict next most likely token\n",
    "with torch.no_grad():\n",
    "    input_sent = \"My sister is 20\"\n",
    "    input_ids = torch.tensor(tokenizer.encode(input_sent)).unsqueeze(0)  # Batch size 1\n",
    "    outputs = model(input_ids)\n",
    "    loss, logits = outputs[:2]\n",
    "    predicted_index = torch.argmax(logits[0, -1, :]).item()\n",
    "    predicted_text = tokenizer.decode([predicted_index])\n",
    "    print(\"Voc size\", logits.shape[-1])\n",
    "    print(input_sent, predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "## Your code for initializing the rnn model(s)\n",
    "#\n",
    "# The Gulordava LSTM model can be found here: \n",
    "# https://drive.google.com/open?id=1w47WsZcZzPyBKDn83cMNd0Hb336e-_Sy\n",
    "#\n",
    "# N.B: I have altered the RNNModel code to only output the hidden states that you are interested in.\n",
    "# If you want to do more experiments with this model you could have a look at the original code here:\n",
    "# https://github.com/facebookresearch/colorlessgreenRNNs/blob/master/src/language_models/model.py\n",
    "#\n",
    "from collections import defaultdict\n",
    "from lstm.model import RNNModel\n",
    "\n",
    "model_location = 'lstm/gulordava.pt'\n",
    "lstm = RNNModel('LSTM', 50001, 650, 650, 2)\n",
    "lstm.load_state_dict(torch.load(model_location))\n",
    "\n",
    "\n",
    "# This LSTM does not use a Tokenizer like the Transformers, but a Vocab dictionary that maps a token to an id.\n",
    "with open('lstm/vocab.txt') as f:\n",
    "    w2i = {w.strip(): i for i, w in enumerate(f)}\n",
    "\n",
    "vocab = defaultdict(lambda: w2i[\"<unk>\"])\n",
    "vocab.update(w2i)\n",
    "i2w = { w2i[k]:k for k in w2i}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'lstm.model.RNNModel'>\n",
      "Voc size 50001\n",
      "My sister is 20 years\n"
     ]
    }
   ],
   "source": [
    "lstm.eval()\n",
    "print(type(lstm))\n",
    "with torch.no_grad():\n",
    "    input_sent = \"My sister is 20\"\n",
    "    inputs = torch.tensor([vocab[z] for z in input_sent.split()]).unsqueeze(0)\n",
    "    outputs = lstm(inputs, lstm.init_hidden(1), True)\n",
    "    argmaxes = torch.argmax(outputs, dim=1)\n",
    "    last = argmaxes[-1]\n",
    "    print(\"Voc size\", outputs.shape[-1])\n",
    "    print(input_sent, i2w[last.item()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good idea that before you move on, you try to feed some text to your LMs; and check if everything works accordingly. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "For this assignment you will train your probes on __treebank__ corpora. A treebank is a corpus that has been *parsed*, and stored in a representation that allows the parse tree to be recovered. Next to a parse tree, treebanks also often contain information about part-of-speech tags, which is exactly what we are after now.\n",
    "\n",
    "The treebank you will use for now is part of the Universal Dependencies project. I provide a sample of this treebank as well, so you can test your setup on that before moving on to larger amounts of data.\n",
    "\n",
    "Make sure you accustom yourself to the format that is created by the `conllu` library that parses the treebank files before moving on. For example, make sure you understand how you can access the pos tag of a token, or how to cope with the tree structure that is formed using the `to_tree()` functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA\n",
    "from typing import List\n",
    "from conllu import parse_incr, TokenList\n",
    "\n",
    "\n",
    "# If stuff like `: str` and `-> ..` seems scary, fear not! \n",
    "# These are type hints that help you to understand what kind of argument and output is expected.\n",
    "def parse_corpus(filename: str) -> List[TokenList]:\n",
    "    data_file = open(filename, encoding=\"utf-8\")\n",
    "\n",
    "    ud_parses = list(parse_incr(data_file))\n",
    "    \n",
    "    return ud_parses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Representations\n",
    "\n",
    "We now have our data all set, our models are running and we are good to go!\n",
    "\n",
    "The next step is now to create the model representations for the sentences in our corpora. Once we have generated these representations we can store them, and train additional diagnostic (/probing) classifiers on top of the representations.\n",
    "\n",
    "There are a few things you should keep in mind here. Read these carefully, as these tips will save you a lot of time in your implementation.\n",
    "- Transformer models make use of Byte-Pair Encodings (BPE), that chunk up a piece of next in subword pieces. For example, a word such as \"largely\" could be chunked up into \"large\" and \"ly\". We are interested in probing linguistic information on the __word__-level. Therefore, we will follow the suggestion of Hewitt et al. (2019a, footnote 4), and create the representation of a word by averaging over the representations of its subwords. So the representation of \"largely\" becomes the average of that of \"large\" and \"ly\".\n",
    "\n",
    "- Subword chunks never overlap multiple tokens. In other words, say we have a phrase like \"None of the\", then the tokenizer might chunk that into \"No\"+\"ne\"+\" of\"+\" the\", but __not__ into \"No\"+\"ne o\"+\"f the\", as those chunks overlap multiple tokens. This is great for our setup! Otherwise it would have been quite challenging to distribute the representation of a subword over the 2 tokens it belongs to.\n",
    "\n",
    "- If you closely examine the provided treebank, you will notice that some tokens are split up into multiple pieces, that each have their own POS-tag. For example, in the first sentence the word \"Al-Zaman\" is split into \"Al\", \"-\", and \"Zaman\". In such cases, the conllu `TokenList` format will add the following attribute: `('misc', OrderedDict([('SpaceAfter', 'No')]))` to these tokens. Your model's tokenizer does not need to adhere to the same tokenization. E.g., \"Al-Zaman\" could be split into \"Al-\"+\"Za\"+\"man\", making it hard to match the representations with their correct pos-tag. Therefore I recommend you to not tokenize your entire sentence at once, but to do this based on the chunking of the treebank. Make sure to still incoporate the spaces in a sentence though, as these are part of the BPE of the tokenizer. The tokenizer for GPT-2 adds spaces at the start of a token.\n",
    "\n",
    "- The LSTM LM does not have the issues related to subwords, but is far more restricted in its vocabulary. Make sure you keep the above points in mind though, when creating the LSTM representations. You might want to write separate functions for the LSTM, but that is up to you.\n",
    "\n",
    "I would like to stress that if you feel hindered in any way by the simple code structure that is presented here, you are free to modify it :-) Just make sure it is clear to an outsider what you're doing, some helpful comments never hurt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90\n",
      "['Two ', 'other ', 'Dulaim ', 'leaders ', 'have ', 'been ', 'killed ', 'in ', 'the ', 'past ', 'week ', 'and ', 'a ', 'half', '. ']\n",
      "['Two', ' other', ' Dul', 'aim', ' leaders', ' have', ' been', ' killed', ' in', ' the', ' past', ' week', ' and', ' a', ' half', '.']\n",
      "['Guerrillas ', 'near ', 'Hawijah ', 'launched ', 'an ', 'attack ', 'that ', 'left ', '6 ', 'dead', ', ', 'including ', '4 ', 'Iraqi ', 'soldiers', '. ']\n",
      "['Gu', 'err', 'illas', ' near', ' Haw', 'ijah', ' launched', ' an', ' attack', ' that', ' left', ' 6', ' dead', ',', ' including', ' 4', ' Iraqi', ' soldiers', '.']\n",
      "['One ', 'of ', 'them ', 'was ', 'from ', 'the ', 'Jubur ', 'tribe ', 'and ', 'was ', 'deputy ', 'commander ', 'of ', 'the ', 'Hawijah ', 'garrison', '. ']\n",
      "['One', ' of', ' them', ' was', ' from', ' the', ' Jub', 'ur', ' tribe', ' and', ' was', ' deputy', ' commander', ' of', ' the', ' Haw', 'ijah', ' garrison', '.']\n",
      "['Two ', 'hundred ', 'members ', 'of ', 'the ', 'Batawi ', 'clan ', 'of ', 'the ', 'Dulaim ', 'demonstrated ', 'in ', 'Baghdad ', 'on ', 'Friday', ', ', 'protesting ', 'the ', 'killing ', 'of ', 'their ', 'clan ', 'elder', ', ', 'Shaikh ', 'Kadhim ', 'Sarhid ', 'and ', '4 ', 'of ', 'his ', 'sons', ', ', 'by ', 'gunmen ', 'wearing ', 'Iraqi ', 'army ', 'uniforms', '. ']\n",
      "['Two', ' hundred', ' members', ' of', ' the', ' Bat', 'awi', ' clan', ' of', ' the', ' Dul', 'aim', ' demonstrated', ' in', ' Baghdad', ' on', ' Friday', ',', ' protesting', ' the', ' killing', ' of', ' their', ' clan', ' elder', ',', ' Sha', 'ikh', ' K', 'adh', 'im', ' Sar', 'hid', ' and', ' 4', ' of', ' his', ' sons', ',', ' by', ' gunmen', ' wearing', ' Iraqi', ' army', ' uniforms', '.']\n",
      "0 46 looking for Two \n",
      "1 46 looking for hundred \n",
      "2 46 looking for members \n",
      "3 46 looking for of \n",
      "4 46 looking for the \n",
      "5 46 looking for Batawi \n",
      "7 46 looking for clan \n",
      "8 46 looking for of \n",
      "9 46 looking for the \n",
      "10 46 looking for Dulaim \n",
      "12 46 looking for demonstrated \n",
      "13 46 looking for in \n",
      "14 46 looking for Baghdad \n",
      "15 46 looking for on \n",
      "16 46 looking for Friday\n",
      "17 46 looking for , \n",
      "18 46 looking for protesting \n",
      "19 46 looking for the \n",
      "20 46 looking for killing \n",
      "21 46 looking for of \n",
      "22 46 looking for their \n",
      "23 46 looking for clan \n",
      "24 46 looking for elder\n",
      "25 46 looking for , \n",
      "26 46 looking for Shaikh \n",
      "28 46 looking for Kadhim \n",
      "31 46 looking for Sarhid \n",
      "33 46 looking for and \n",
      "34 46 looking for 4 \n",
      "35 46 looking for of \n",
      "36 46 looking for his \n",
      "37 46 looking for sons\n",
      "38 46 looking for , \n",
      "39 46 looking for by \n",
      "40 46 looking for gunmen \n",
      "41 46 looking for wearing \n",
      "42 46 looking for Iraqi \n",
      "43 46 looking for army \n",
      "44 46 looking for uniforms\n",
      "45 46 looking for . \n",
      "['This ', 'is ', 'a ', 'largely ', 'Sunni ', 'Arab ', 'clan', ', ', 'and ', 'some ', 'Sunni ', 'observers ', 'have ', 'accused ', 'Shiite ', 'elements ', 'in ', 'the ', 'government ', 'of ', 'being ', 'behind ', 'the ', 'assassination', '; ', 'it ', 'is ', 'more ', 'likely ', 'the ', 'work ', 'of ', 'Sunni ', 'Arab ', 'guerrillas ', 'punishing ', 'the ', 'Batawi ', 'leaders ', 'for ', 'cooperating ', 'with ', 'the ', 'Dec. ', '15 ', 'elections', '.']\n",
      "['This', ' is', ' a', ' largely', ' Sunni', ' Arab', ' clan', ',', ' and', ' some', ' Sunni', ' observers', ' have', ' accused', ' Shiite', ' elements', ' in', ' the', ' government', ' of', ' being', ' behind', ' the', ' assassination', ';', ' it', ' is', ' more', ' likely', ' the', ' work', ' of', ' Sunni', ' Arab', ' gu', 'err', 'illas', ' punishing', ' the', ' Bat', 'awi', ' leaders', ' for', ' cooperating', ' with', ' the', ' Dec', '.', ' 15', ' elections', '.']\n",
      "0 51 looking for This \n",
      "1 51 looking for is \n",
      "2 51 looking for a \n",
      "3 51 looking for largely \n",
      "4 51 looking for Sunni \n",
      "5 51 looking for Arab \n",
      "6 51 looking for clan\n",
      "7 51 looking for , \n",
      "8 51 looking for and \n",
      "9 51 looking for some \n",
      "10 51 looking for Sunni \n",
      "11 51 looking for observers \n",
      "12 51 looking for have \n",
      "13 51 looking for accused \n",
      "14 51 looking for Shiite \n",
      "15 51 looking for elements \n",
      "16 51 looking for in \n",
      "17 51 looking for the \n",
      "18 51 looking for government \n",
      "19 51 looking for of \n",
      "20 51 looking for being \n",
      "21 51 looking for behind \n",
      "22 51 looking for the \n",
      "23 51 looking for assassination\n",
      "24 51 looking for ; \n",
      "25 51 looking for it \n",
      "26 51 looking for is \n",
      "27 51 looking for more \n",
      "28 51 looking for likely \n",
      "29 51 looking for the \n",
      "30 51 looking for work \n",
      "31 51 looking for of \n",
      "32 51 looking for Sunni \n",
      "33 51 looking for Arab \n",
      "34 51 looking for guerrillas \n",
      "37 51 looking for punishing \n",
      "38 51 looking for the \n",
      "39 51 looking for Batawi \n",
      "41 51 looking for leaders \n",
      "42 51 looking for for \n",
      "43 51 looking for cooperating \n",
      "44 51 looking for with \n",
      "45 51 looking for the \n",
      "46 51 looking for Dec. \n",
      "46 51 looking for 15 \n",
      "46 51 looking for elections\n",
      "46 51 looking for .\n",
      "['Al', '-', 'Zaman ', ': ', 'The ', 'Iraqi ', 'High ', 'Electoral ', 'Commission ', 'on ', 'Friday ', 'denied ', 'a ', 'request ', 'of ', 'the ', 'Debaathification ', 'Commission ', 'to ', 'exclude ', '51 ', 'individuals ', 'from ', 'running ', 'on ', 'party ', 'lists ', 'in ', 'the ', 'Dec. ', '15 ', 'elections ', 'on ', 'grounds ', 'of ', 'having ', 'been ', 'sufficiently ', 'involved ', 'in ', 'Baath ', 'activities ', 'to ', 'warrant ', 'their ', 'being ', 'excluded ', 'from ', 'civil ', 'office', '. ']\n",
      "['Al', '-', 'Z', 'aman', ' :', ' The', ' Iraqi', ' High', ' Electoral', ' Commission', ' on', ' Friday', ' denied', ' a', ' request', ' of', ' the', ' De', 'ba', 'ath', 'ification', ' Commission', ' to', ' exclude', ' 51', ' individuals', ' from', ' running', ' on', ' party', ' lists', ' in', ' the', ' Dec', '.', ' 15', ' elections', ' on', ' grounds', ' of', ' having', ' been', ' sufficiently', ' involved', ' in', ' Ba', 'ath', ' activities', ' to', ' warrant', ' their', ' being', ' excluded', ' from', ' civil', ' office', '.']\n",
      "0 57 looking for Al\n",
      "1 57 looking for -\n",
      "2 57 looking for Zaman \n",
      "4 57 looking for : \n",
      "5 57 looking for The \n",
      "6 57 looking for Iraqi \n",
      "7 57 looking for High \n",
      "8 57 looking for Electoral \n",
      "9 57 looking for Commission \n",
      "10 57 looking for on \n",
      "11 57 looking for Friday \n",
      "12 57 looking for denied \n",
      "13 57 looking for a \n",
      "14 57 looking for request \n",
      "15 57 looking for of \n",
      "16 57 looking for the \n",
      "17 57 looking for Debaathification \n",
      "21 57 looking for Commission \n",
      "22 57 looking for to \n",
      "23 57 looking for exclude \n",
      "24 57 looking for 51 \n",
      "25 57 looking for individuals \n",
      "26 57 looking for from \n",
      "27 57 looking for running \n",
      "28 57 looking for on \n",
      "29 57 looking for party \n",
      "30 57 looking for lists \n",
      "31 57 looking for in \n",
      "32 57 looking for the \n",
      "33 57 looking for Dec. \n",
      "35 57 looking for 15 \n",
      "36 57 looking for elections \n",
      "37 57 looking for on \n",
      "38 57 looking for grounds \n",
      "39 57 looking for of \n",
      "40 57 looking for having \n",
      "41 57 looking for been \n",
      "42 57 looking for sufficiently \n",
      "43 57 looking for involved \n",
      "44 57 looking for in \n",
      "45 57 looking for Baath \n",
      "47 57 looking for activities \n",
      "48 57 looking for to \n",
      "49 57 looking for warrant \n",
      "50 57 looking for their \n",
      "51 57 looking for being \n",
      "52 57 looking for excluded \n",
      "53 57 looking for from \n",
      "54 57 looking for civil \n",
      "55 57 looking for office\n",
      "56 57 looking for . \n",
      "['The ', 'Commission ', 'said ', 'it ', 'had ', 'no ', 'legal ', 'grounds ', 'for ', 'such ', 'an ', 'exclusion', '. ']\n",
      "['The', ' Commission', ' said', ' it', ' had', ' no', ' legal', ' grounds', ' for', ' such', ' an', ' exclusion', '.']\n",
      "['This ', 'item ', 'is ', 'a ', 'small ', 'one ', 'and ', 'easily ', 'missed', '. ']\n",
      "['This', ' item', ' is', ' a', ' small', ' one', ' and', ' easily', ' missed', '.']\n",
      "['But ', 'in ', 'my ', 'view ', 'it ', 'is ', 'highly ', 'significant', '. ']\n",
      "['But', ' in', ' my', ' view', ' it', ' is', ' highly', ' significant', '.']\n",
      "['The ', 'Debaathification ', 'Commission ', 'had ', 'been ', 'pushed ', 'by ', 'Ahmad ', 'Chalabi ', 'and ', 'his ', 'Iraqi ', 'National ', 'Congress ', 'very ', 'hard', ', ', 'and ', 'had ', 'pushed ', 'many ', 'Sunni ', 'Arabs ', 'into ', 'the ', 'arms ', 'of ', 'the ', 'guerrillas', '. ']\n",
      "['The', ' De', 'ba', 'ath', 'ification', ' Commission', ' had', ' been', ' pushed', ' by', ' Ahmad', ' Chal', 'abi', ' and', ' his', ' Iraqi', ' National', ' Congress', ' very', ' hard', ',', ' and', ' had', ' pushed', ' many', ' Sunni', ' Arabs', ' into', ' the', ' arms', ' of', ' the', ' gu', 'err', 'illas', '.']\n",
      "['Chalabi ', 'has ', 'been ', 'increasingly ', 'marginalized ', 'within ', 'Iraq', ', ', 'however', ', ', 'despite ', 'his ', 'ties ', 'of ', 'clientelage ', 'with ', 'Washington ', 'and ', 'Tehran', '. ']\n",
      "['Ch', 'al', 'abi', ' has', ' been', ' increasingly', ' marginalized', ' within', ' Iraq', ',', ' however', ',', ' despite', ' his', ' ties', ' of', ' client', 'el', 'age', ' with', ' Washington', ' and', ' Tehran', '.']\n",
      "['He ', 'is ', 'no ', 'longer ', 'in ', 'the ', 'dominant ', 'Shiite ', 'list', ', ', 'the ', 'United ', 'Iraqi ', 'Alliance', ', ', 'and ', 'wo', \"n't \", 'have ', 'many ', 'seats ', 'in ', 'the ', 'new ', 'parliament', '. ']\n",
      "['He', ' is', ' no', ' longer', ' in', ' the', ' dominant', ' Shiite', ' list', ',', ' the', ' United', ' Iraqi', ' Alliance', ',', ' and', ' won', \"'t\", ' have', ' many', ' seats', ' in', ' the', ' new', ' parliament', '.']\n",
      "['Some ', '2,000 ', 'junior ', 'officers ', 'of ', 'the ', 'old ', 'Baath ', 'army ', 'have ', 'been ', 'recalled ', 'to ', 'duty ', 'in ', 'recent ', 'months', ', ', 'something ', 'Chalabi ', 'would ', 'have ', 'blocked ', 'if ', 'he ', 'could ', 'have', '. ']\n",
      "['Some', ' 2', ',', '000', ' junior', ' officers', ' of', ' the', ' old', ' Ba', 'ath', ' army', ' have', ' been', ' recalled', ' to', ' duty', ' in', ' recent', ' months', ',', ' something', ' Chal', 'abi', ' would', ' have', ' blocked', ' if', ' he', ' could', ' have', '.']\n",
      "['Now ', 'the ', 'Electoral ', 'Commission ', 'is ', 'refusing ', 'to ', 'punish ', 'people ', 'for ', 'mere ', 'past ', 'Baath ', 'Party ', 'membership', '. ']\n",
      "['Now', ' the', ' Electoral', ' Commission', ' is', ' refusing', ' to', ' punish', ' people', ' for', ' mere', ' past', ' Ba', 'ath', ' Party', ' membership', '.']\n",
      "['The ', 'situation ', 'in ', 'Iraq ', 'is ', 'only ', 'going ', 'to ', 'get ', 'better ', 'this ', 'way', '. ']\n",
      "['The', ' situation', ' in', ' Iraq', ' is', ' only', ' going', ' to', ' get', ' better', ' this', ' way', '.']\n",
      "['If ', 'someone ', 'committed ', 'a ', 'crime ', 'against ', 'humanity', ', ', 'prosecute ', 'the ', 'person', '. ']\n",
      "['If', ' someone', ' committed', ' a', ' crime', ' against', ' humanity', ',', ' prosecute', ' the', ' person', '.']\n",
      "['If ', 'he ', 'or ', 'she ', 'did ', 'not', ', ', 'then ', 'they ', 'should ', 'have ', 'all ', 'the ', 'same ', 'rights ', 'as ', 'other ', 'Iraqis', '. ']\n",
      "['If', ' he', ' or', ' she', ' did', ' not', ',', ' then', ' they', ' should', ' have', ' all', ' the', ' same', ' rights', ' as', ' other', ' Iraqis', '.']\n",
      "['Al', '-', 'Sharq ', 'al', '-', 'Awsat ', 'reports ', 'that ', 'a ', 'key ', 'eyewitness ', 'in ', 'the ', 'trial ', 'of ', 'Saddam ', 'Hussein ', 'for ', 'a ', '1982 ', 'massacre ', 'at ', 'Dujail ', 'has ', 'died', '. ']\n",
      "['Al', '-', 'Shar', 'q', ' al', '-', 'A', 'ws', 'at', ' reports', ' that', ' a', ' key', ' eyewitness', ' in', ' the', ' trial', ' of', ' Saddam', ' Hussein', ' for', ' a', ' 1982', ' massacre', ' at', ' Du', 'j', 'ail', ' has', ' died', '.']\n",
      "['A ', 'team ', 'from ', 'the ', 'court ', 'managed ', 'to ', 'take ', 'his ', 'deposition ', 'before ', 'he ', 'died', '. ']\n",
      "['A', ' team', ' from', ' the', ' court', ' managed', ' to', ' take', ' his', ' deposition', ' before', ' he', ' died', '.']\n",
      "['The ', 'trial ', 'begins ', 'again ', 'Nov.', '28', '. ']\n",
      "['The', ' trial', ' begins', ' again', ' Nov', '.', '28', '.']\n",
      "['In ', 'Baghdad ', 'the ', 'fighting ', 'still ', 'continues ', 'in ', 'several ', 'areas', ', ', 'mostly ', 'in ', 'Sadr ', 'city ', 'and ', 'Adhamiya', '. ']\n",
      "['In', ' Baghdad', ' the', ' fighting', ' still', ' continues', ' in', ' several', ' areas', ',', ' mostly', ' in', ' Sad', 'r', ' city', ' and', ' Ad', 'ham', 'iya', '.']\n",
      "['Baghdadis ', 'do', \"n't \", 'venture ', 'much ', 'out ', 'of ', 'their ', 'neighbourhoods ', 'any ', 'more', ', ', 'you ', 'never ', 'know ', 'where ', 'you ', 'might ', 'get ', 'stuck', '. ']\n",
      "['B', 'aghd', 'ad', 'is', ' don', \"'t\", ' venture', ' much', ' out', ' of', ' their', ' neighbourhoods', ' any', ' more', ',', ' you', ' never', ' know', ' where', ' you', ' might', ' get', ' stuck', '.']\n",
      "['There ', 'has ', 'been ', 'talk ', 'that ', 'the ', 'night ', 'curfew ', 'might ', 'be ', 'implemented ', 'again', '. ']\n",
      "['There', ' has', ' been', ' talk', ' that', ' the', ' night', ' curfew', ' might', ' be', ' implemented', ' again', '.']\n",
      "['My ', 'neighbourhood ', 'has ', 'been ', 'surrounded ', 'by ', 'American ', 'troops ', 'for ', 'three ', 'days ', 'now', ', ', 'helicopters ', 'have ', 'been ', 'circling ', 'over ', 'our ', 'heads ', 'non-stop', '. ']\n",
      "['My', ' neighbourhood', ' has', ' been', ' surrounded', ' by', ' American', ' troops', ' for', ' three', ' days', ' now', ',', ' helicopters', ' have', ' been', ' circling', ' over', ' our', ' heads', ' non', '-', 'stop', '.']\n",
      "['Fedayeen ', 'are ', 'now ', 'visible ', 'on ', 'the ', 'street ', 'and ', 'they ', 'have ', 'become ', 'bolder ', 'than ', 'ever', '. ']\n",
      "['F', 'eday', 'een', ' are', ' now', ' visible', ' on', ' the', ' street', ' and', ' they', ' have', ' become', ' bold', 'er', ' than', ' ever', '.']\n",
      "['Yesterday ', 'there ', 'were ', 'tens ', 'of ', 'them ', 'putting ', 'road ', 'blocks ', 'on ', 'our ', 'street ', 'and ', 'setting ', 'up ', 'mortars', ', ', 'they ', 'only ', 'come ', 'out ', 'in ', 'the ', 'open ', 'when ', 'Americans ', 'leave ', 'the ', 'area', ', ', 'then ', 'they ', 'start ', 'firing ', 'mortars ', 'indiscriminately ', 'and ', 'shooting ', 'their ', 'AK', '-', \"47's \", 'in ', 'the ', 'air', '. ']\n",
      "['Yesterday', ' there', ' were', ' tens', ' of', ' them', ' putting', ' road', ' blocks', ' on', ' our', ' street', ' and', ' setting', ' up', ' mort', 'ars', ',', ' they', ' only', ' come', ' out', ' in', ' the', ' open', ' when', ' Americans', ' leave', ' the', ' area', ',', ' then', ' they', ' start', ' firing', ' mort', 'ars', ' indiscrim', 'inately', ' and', ' shooting', ' their', ' AK', '-', '47', \"'s\", ' in', ' the', ' air', '.']\n",
      "0 50 looking for Yesterday \n",
      "1 50 looking for there \n",
      "2 50 looking for were \n",
      "3 50 looking for tens \n",
      "4 50 looking for of \n",
      "5 50 looking for them \n",
      "6 50 looking for putting \n",
      "7 50 looking for road \n",
      "8 50 looking for blocks \n",
      "9 50 looking for on \n",
      "10 50 looking for our \n",
      "11 50 looking for street \n",
      "12 50 looking for and \n",
      "13 50 looking for setting \n",
      "14 50 looking for up \n",
      "15 50 looking for mortars\n",
      "17 50 looking for , \n",
      "18 50 looking for they \n",
      "19 50 looking for only \n",
      "20 50 looking for come \n",
      "21 50 looking for out \n",
      "22 50 looking for in \n",
      "23 50 looking for the \n",
      "24 50 looking for open \n",
      "25 50 looking for when \n",
      "26 50 looking for Americans \n",
      "27 50 looking for leave \n",
      "28 50 looking for the \n",
      "29 50 looking for area\n",
      "30 50 looking for , \n",
      "31 50 looking for then \n",
      "32 50 looking for they \n",
      "33 50 looking for start \n",
      "34 50 looking for firing \n",
      "35 50 looking for mortars \n",
      "37 50 looking for indiscriminately \n",
      "39 50 looking for and \n",
      "40 50 looking for shooting \n",
      "41 50 looking for their \n",
      "42 50 looking for AK\n",
      "43 50 looking for -\n",
      "44 50 looking for 47's \n",
      "45 50 looking for in \n",
      "45 50 looking for the \n",
      "45 50 looking for air\n",
      "45 50 looking for . \n",
      "['They ', 'are ', 'setting ', 'the ', 'road ', 'blocks ', 'at ', 'the ', 'exact ', 'same ', 'positions ', 'they ', 'were ', 'during ', 'the ', 'war ', 'last ', 'year', ', ', 'which ', 'indicates ', 'they ', 'are ', 'the ', 'same ', 'people', '. ']\n",
      "['They', ' are', ' setting', ' the', ' road', ' blocks', ' at', ' the', ' exact', ' same', ' positions', ' they', ' were', ' during', ' the', ' war', ' last', ' year', ',', ' which', ' indicates', ' they', ' are', ' the', ' same', ' people', '.']\n",
      "['And ', 'there ', 'is ', 'nothing ', 'we ', 'can ', 'do ', 'about ', 'it ', 'really', ', ', 'people ', 'who ', 'are ', 'suggesting ', 'that ', 'we ', 'go ', 'out ', 'and ', 'fight ', 'them ', 'are ', 'living ', 'in ', 'dream ', 'land', '. ']\n",
      "['And', ' there', ' is', ' nothing', ' we', ' can', ' do', ' about', ' it', ' really', ',', ' people', ' who', ' are', ' suggesting', ' that', ' we', ' go', ' out', ' and', ' fight', ' them', ' are', ' living', ' in', ' dream', ' land', '.']\n",
      "['Even ', 'the ', 'IP ', 'and ', 'ICDC ', 'have ', 'abandoned ', 'the ', 'neighbourhood', ', ', 'and ', 'those ', 'are ', 'trained ', 'and ', 'armed', ', ', 'so ', 'do', \"n't \", 'expect ', 'scared ', 'civilians ', 'to ', 'do ', 'anything ', 'except ', 'to ', 'hide ', 'inside ', 'and ', 'pray ', 'a ', 'helicopter ', 'or ', 'a ', 'tank ', 'does', \"n't \", 'bomb ', 'them', ', ', 'and ', 'also ', 'how ', 'are ', 'American ', 'soldiers ', 'going ', 'to ', 'distinguish ', 'the ', 'brave ', 'and ', 'valiant ', 'civilians ', 'from ', 'the ', 'Fedayeen', '? ']\n",
      "['Even', ' the', ' IP', ' and', ' I', 'CDC', ' have', ' abandoned', ' the', ' neighbourhood', ',', ' and', ' those', ' are', ' trained', ' and', ' armed', ',', ' so', ' don', \"'t\", ' expect', ' scared', ' civilians', ' to', ' do', ' anything', ' except', ' to', ' hide', ' inside', ' and', ' pray', ' a', ' helicopter', ' or', ' a', ' tank', ' doesn', \"'t\", ' bomb', ' them', ',', ' and', ' also', ' how', ' are', ' American', ' soldiers', ' going', ' to', ' distinguish', ' the', ' brave', ' and', ' valiant', ' civilians', ' from', ' the', ' Fed', 'ay', 'een', '?']\n",
      "0 63 looking for Even \n",
      "1 63 looking for the \n",
      "2 63 looking for IP \n",
      "3 63 looking for and \n",
      "4 63 looking for ICDC \n",
      "6 63 looking for have \n",
      "7 63 looking for abandoned \n",
      "8 63 looking for the \n",
      "9 63 looking for neighbourhood\n",
      "10 63 looking for , \n",
      "11 63 looking for and \n",
      "12 63 looking for those \n",
      "13 63 looking for are \n",
      "14 63 looking for trained \n",
      "15 63 looking for and \n",
      "16 63 looking for armed\n",
      "17 63 looking for , \n",
      "18 63 looking for so \n",
      "19 63 looking for do\n",
      "20 63 looking for n't \n",
      "21 63 looking for expect \n",
      "22 63 looking for scared \n",
      "23 63 looking for civilians \n",
      "24 63 looking for to \n",
      "25 63 looking for do \n",
      "26 63 looking for anything \n",
      "27 63 looking for except \n",
      "28 63 looking for to \n",
      "29 63 looking for hide \n",
      "30 63 looking for inside \n",
      "31 63 looking for and \n",
      "32 63 looking for pray \n",
      "33 63 looking for a \n",
      "34 63 looking for helicopter \n",
      "35 63 looking for or \n",
      "36 63 looking for a \n",
      "37 63 looking for tank \n",
      "38 63 looking for does\n",
      "39 63 looking for n't \n",
      "40 63 looking for bomb \n",
      "41 63 looking for them\n",
      "42 63 looking for , \n",
      "43 63 looking for and \n",
      "44 63 looking for also \n",
      "45 63 looking for how \n",
      "46 63 looking for are \n",
      "47 63 looking for American \n",
      "48 63 looking for soldiers \n",
      "49 63 looking for going \n",
      "50 63 looking for to \n",
      "51 63 looking for distinguish \n",
      "52 63 looking for the \n",
      "53 63 looking for brave \n",
      "54 63 looking for and \n",
      "55 63 looking for valiant \n",
      "56 63 looking for civilians \n",
      "57 63 looking for from \n",
      "58 63 looking for the \n",
      "59 63 looking for Fedayeen\n",
      "59 63 looking for ? \n",
      "['Everyone ', 'is ', 'apprehensive', ', ', 'there ', 'is ', 'some ', 'talk ', 'that ', 'April ', '9th ', 'and ', '10th ', 'are ', 'going ', 'to ', 'be ', 'bloody ', 'days', '. ']\n",
      "['Everyone', ' is', ' apprehens', 'ive', ',', ' there', ' is', ' some', ' talk', ' that', ' April', ' 9', 'th', ' and', ' 10', 'th', ' are', ' going', ' to', ' be', ' bloody', ' days', '.']\n",
      "['Most ', 'people ', 'have', \"n't \", 'gone ', 'to ', 'work ', 'the ', 'last ', 'few ', 'days', ', ', 'although ', 'it ', 'seems ', 'that ', 'the ', 'rest ', 'of ', 'Baghdad ', 'is ', \"'\", 'normal', \"' \", 'if ', 'you ', 'can ', 'define ', 'what ', 'normal ', 'is', '. ']\n",
      "['Most', ' people', ' haven', \"'t\", ' gone', ' to', ' work', ' the', ' last', ' few', ' days', ',', ' although', ' it', ' seems', ' that', ' the', ' rest', ' of', ' Baghdad', ' is', \" '\", 'normal', \"'\", ' if', ' you', ' can', ' define', ' what', ' normal', ' is', '.']\n",
      "['There ', 'are ', 'rumours ', 'about ', 'preparations ', 'by ', 'slum ', 'dwellers ', 'for ', 'another ', 'looting ', 'spree ', 'against ', 'banks', ', ', 'governmental ', 'and ', 'public ', 'property ', 'similar ', 'to ', 'the ', 'one ', 'that ', 'took ', 'place ', 'last ', 'April', ', ', 'and ', 'I ', 'have ', 'already ', 'overheard ', 'youngsters ', 'in ', 'my ', 'neighbourhood ', 'joking ', 'about ', 'it ', 'and ', 'saying ', 'things ', 'like ', '\"', 'This ', 'time ', 'we ', 'will ', 'be ', 'the ', 'first ', 'to ', 'loot', ', ', 'we ', 'did', \"n't \", 'get ', 'anything ', 'the ', 'last ', 'time', '\"', '. ']\n",
      "['There', ' are', ' rumours', ' about', ' preparations', ' by', ' sl', 'um', ' dwell', 'ers', ' for', ' another', ' looting', ' spree', ' against', ' banks', ',', ' governmental', ' and', ' public', ' property', ' similar', ' to', ' the', ' one', ' that', ' took', ' place', ' last', ' April', ',', ' and', ' I', ' have', ' already', ' overheard', ' youngsters', ' in', ' my', ' neighbourhood', ' joking', ' about', ' it', ' and', ' saying', ' things', ' like', ' \"', 'This', ' time', ' we', ' will', ' be', ' the', ' first', ' to', ' loot', ',', ' we', ' didn', \"'t\", ' get', ' anything', ' the', ' last', ' time', '\".']\n",
      "0 67 looking for There \n",
      "1 67 looking for are \n",
      "2 67 looking for rumours \n",
      "3 67 looking for about \n",
      "4 67 looking for preparations \n",
      "5 67 looking for by \n",
      "6 67 looking for slum \n",
      "8 67 looking for dwellers \n",
      "10 67 looking for for \n",
      "11 67 looking for another \n",
      "12 67 looking for looting \n",
      "13 67 looking for spree \n",
      "14 67 looking for against \n",
      "15 67 looking for banks\n",
      "16 67 looking for , \n",
      "17 67 looking for governmental \n",
      "18 67 looking for and \n",
      "19 67 looking for public \n",
      "20 67 looking for property \n",
      "21 67 looking for similar \n",
      "22 67 looking for to \n",
      "23 67 looking for the \n",
      "24 67 looking for one \n",
      "25 67 looking for that \n",
      "26 67 looking for took \n",
      "27 67 looking for place \n",
      "28 67 looking for last \n",
      "29 67 looking for April\n",
      "30 67 looking for , \n",
      "31 67 looking for and \n",
      "32 67 looking for I \n",
      "33 67 looking for have \n",
      "34 67 looking for already \n",
      "35 67 looking for overheard \n",
      "36 67 looking for youngsters \n",
      "37 67 looking for in \n",
      "38 67 looking for my \n",
      "39 67 looking for neighbourhood \n",
      "40 67 looking for joking \n",
      "41 67 looking for about \n",
      "42 67 looking for it \n",
      "43 67 looking for and \n",
      "44 67 looking for saying \n",
      "45 67 looking for things \n",
      "46 67 looking for like \n",
      "47 67 looking for \"\n",
      "48 67 looking for This \n",
      "49 67 looking for time \n",
      "50 67 looking for we \n",
      "51 67 looking for will \n",
      "52 67 looking for be \n",
      "53 67 looking for the \n",
      "54 67 looking for first \n",
      "55 67 looking for to \n",
      "56 67 looking for loot\n",
      "57 67 looking for , \n",
      "58 67 looking for we \n",
      "59 67 looking for did\n",
      "60 67 looking for n't \n",
      "61 67 looking for get \n",
      "62 67 looking for anything \n",
      "63 67 looking for the \n",
      "64 67 looking for last \n",
      "65 67 looking for time\n",
      "66 67 looking for \"\n",
      "66 67 looking for . \n",
      "['Mosques ', 'are ', 'calling ', 'for ', 'donating ', 'blood', ', ', 'food', ', ', 'and ', 'medicine ', 'for ', 'Fallujah', ', ', 'and ', 'several ', 'convoys ', 'have ', 'already ', 'headed ', 'out ', 'for ', 'Fallujah', ', ', 'most ', 'of ', 'them ', 'returned ', 'later ', 'though', '. ']\n",
      "['Mos', 'ques', ' are', ' calling', ' for', ' donating', ' blood', ',', ' food', ',', ' and', ' medicine', ' for', ' Fall', 'ujah', ',', ' and', ' several', ' conv', 'oys', ' have', ' already', ' headed', ' out', ' for', ' Fall', 'ujah', ',', ' most', ' of', ' them', ' returned', ' later', ' though', '.']\n",
      "['What ', 'irritates ', 'me ', 'is ', 'this ', 'sudden ', 'false ', \"'\", 'solidarity', \"' \", 'between ', 'Sunni ', 'and ', \"Shi'ite \", 'clerics', ', ', 'we ', 'all ', 'know ', 'that ', 'they ', 'would ', 'be ', 'glad ', 'to ', 'get ', 'at ', 'each ', 'other', 's ', 'throats ', 'when ', 'they ', 'have ', 'the ', 'chance', ', ', 'and ', 'Shia ', 'clerics ', 'were ', 'describing ', 'Fallujan ', 'insurgents ', 'as ', \"'\", \"Ba'athists\", \"'\", ', ', \"'\", 'Saddamites', \"'\", ', ', \"'\", 'Wahhabis', \"'\", ', ', 'and ', \"'\", 'terrorists', \"' \", 'just ', 'a ', 'few ', 'days ', 'ago', '. ']\n",
      "['What', ' irrit', 'ates', ' me', ' is', ' this', ' sudden', ' false', \" '\", 'solid', 'arity', \"'\", ' between', ' Sunni', ' and', ' Shi', \"'\", 'ite', ' clerics', ',', ' we', ' all', ' know', ' that', ' they', ' would', ' be', ' glad', ' to', ' get', ' at', ' each', ' others', ' throats', ' when', ' they', ' have', ' the', ' chance', ',', ' and', ' Shia', ' clerics', ' were', ' describing', ' Fall', 'u', 'jan', ' insurgents', ' as', \" '\", 'Ba', \"'\", 'ath', 'ists', \"',\", \" '\", 'S', 'add', 'am', 'ites', \"',\", \" '\", 'W', 'ah', 'hab', 'is', \"',\", ' and', \" '\", 'terror', 'ists', \"'\", ' just', ' a', ' few', ' days', ' ago', '.']\n",
      "0 79 looking for What \n",
      "1 79 looking for irritates \n",
      "3 79 looking for me \n",
      "4 79 looking for is \n",
      "5 79 looking for this \n",
      "6 79 looking for sudden \n",
      "7 79 looking for false \n",
      "8 79 looking for '\n",
      "9 79 looking for solidarity\n",
      "11 79 looking for ' \n",
      "12 79 looking for between \n",
      "13 79 looking for Sunni \n",
      "14 79 looking for and \n",
      "15 79 looking for Shi'ite \n",
      "18 79 looking for clerics\n",
      "19 79 looking for , \n",
      "20 79 looking for we \n",
      "21 79 looking for all \n",
      "22 79 looking for know \n",
      "23 79 looking for that \n",
      "24 79 looking for they \n",
      "25 79 looking for would \n",
      "26 79 looking for be \n",
      "27 79 looking for glad \n",
      "28 79 looking for to \n",
      "29 79 looking for get \n",
      "30 79 looking for at \n",
      "31 79 looking for each \n",
      "32 79 looking for other\n",
      "33 79 looking for s \n",
      "34 79 looking for throats \n",
      "66 79 looking for when \n",
      "66 79 looking for they \n",
      "66 79 looking for have \n",
      "66 79 looking for the \n",
      "66 79 looking for chance\n",
      "66 79 looking for , \n",
      "66 79 looking for and \n",
      "66 79 looking for Shia \n",
      "66 79 looking for clerics \n",
      "66 79 looking for were \n",
      "66 79 looking for describing \n",
      "66 79 looking for Fallujan \n",
      "66 79 looking for insurgents \n",
      "66 79 looking for as \n",
      "66 79 looking for '\n",
      "66 79 looking for Ba'athists\n",
      "66 79 looking for '\n",
      "66 79 looking for , \n",
      "66 79 looking for '\n",
      "66 79 looking for Saddamites\n",
      "66 79 looking for '\n",
      "66 79 looking for , \n",
      "66 79 looking for '\n",
      "66 79 looking for Wahhabis\n",
      "66 79 looking for '\n",
      "66 79 looking for , \n",
      "66 79 looking for and \n",
      "66 79 looking for '\n",
      "66 79 looking for terrorists\n",
      "66 79 looking for ' \n",
      "66 79 looking for just \n",
      "66 79 looking for a \n",
      "66 79 looking for few \n",
      "66 79 looking for days \n",
      "66 79 looking for ago\n",
      "66 79 looking for . \n",
      "['So ', 'what ', 'happened', '? ']\n",
      "['So', ' what', ' happened', '?']\n",
      "['I ', 'guess ', 'it', \"'s \", 'just ', 'the ', 'old ', 'new ', 'Arab ', \"'\", 'Me ', 'against ', 'my ', 'brother', ', ', 'me ', 'and ', 'my ', 'brother ', 'against ', 'my ', 'cousin', ', ', 'me ', 'and ', 'my ', 'cousin ', 'against ', 'my ', 'enemy', \"'\", ', ', 'or ', \"'\", 'The ', 'enemy ', 'of ', 'my ', 'enemy ', 'is ', 'my ', 'friend', \"' \", 'thing ', 'going ', 'on ', 'again', '. ']\n",
      "['I', ' guess', ' it', \"'s\", ' just', ' the', ' old', ' new', ' Arab', \" '\", 'Me', ' against', ' my', ' brother', ',', ' me', ' and', ' my', ' brother', ' against', ' my', ' cousin', ',', ' me', ' and', ' my', ' cousin', ' against', ' my', ' enemy', \"',\", ' or', \" '\", 'The', ' enemy', ' of', ' my', ' enemy', ' is', ' my', ' friend', \"'\", ' thing', ' going', ' on', ' again', '.']\n",
      "0 47 looking for I \n",
      "1 47 looking for guess \n",
      "2 47 looking for it\n",
      "3 47 looking for 's \n",
      "4 47 looking for just \n",
      "5 47 looking for the \n",
      "6 47 looking for old \n",
      "7 47 looking for new \n",
      "8 47 looking for Arab \n",
      "9 47 looking for '\n",
      "10 47 looking for Me \n",
      "11 47 looking for against \n",
      "12 47 looking for my \n",
      "13 47 looking for brother\n",
      "14 47 looking for , \n",
      "15 47 looking for me \n",
      "16 47 looking for and \n",
      "17 47 looking for my \n",
      "18 47 looking for brother \n",
      "19 47 looking for against \n",
      "20 47 looking for my \n",
      "21 47 looking for cousin\n",
      "22 47 looking for , \n",
      "23 47 looking for me \n",
      "24 47 looking for and \n",
      "25 47 looking for my \n",
      "26 47 looking for cousin \n",
      "27 47 looking for against \n",
      "28 47 looking for my \n",
      "29 47 looking for enemy\n",
      "30 47 looking for '\n",
      "31 47 looking for , \n",
      "47 47 looking for or \n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-182-07156c3a32b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msentences_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m \u001b[0massert_sen_reps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-174-c9aba1b13fd8>\u001b[0m in \u001b[0;36massert_sen_reps\u001b[0;34m(model, tokenizer, lstm, vocab)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mown_distilgpt2_emb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_sen_reps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0mown_lstm_emb1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfetch_sen_reps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-182-07156c3a32b5>\u001b[0m in \u001b[0;36mfetch_sen_reps\u001b[0;34m(ud_parses, model, tokenizer)\u001b[0m\n\u001b[1;32m     53\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"looking for\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                 \u001b[0;31m# If the words at the position match exactly, just append the representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m                 \u001b[0mdecoded_input_at_position\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdecoded_input_at_position\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtrue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m                     \u001b[0mfinal_repr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from lstm.model import RNNModel\n",
    "\n",
    "\n",
    "def fetch_sen_reps(ud_parses: List[TokenList], model, tokenizer) -> List[Tensor]:\n",
    "    \"\"\"\n",
    "    Returns a list of length len(ud_parses)\n",
    "    \"\"\"\n",
    "    doing_lstm = type(model) == RNNModel\n",
    "    sentences_result = list()\n",
    "    \n",
    "    for sentence in ud_parses:\n",
    "        sentence_words = list()\n",
    "        \n",
    "        # First build string sentence repr with spaces and such\n",
    "        for token in sentence:\n",
    "            if token['misc'] is not None:\n",
    "                # SpaceAfter = False\n",
    "                if token['form'] not in ['[',']','(',')']:\n",
    "                    next_word = token['form']\n",
    "                    sentence_words.append(next_word)\n",
    "            else:\n",
    "                # SpaceAfter = True\n",
    "                if token['form'] not in ['[',']','(',')']:\n",
    "                    next_word = token['form'] + ' '\n",
    "                    sentence_words.append(next_word)\n",
    "            \n",
    "        # Now build model representation!\n",
    "        \n",
    "        # In case of LSTM\n",
    "        if doing_lstm:\n",
    "            the_input = torch.tensor([tokenizer[z.strip()] for z in sentence_words]).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                final = model(the_input, lstm.init_hidden(1))\n",
    "            sentences_result.append(final.squeeze(0))\n",
    "        # In case of Transformer\n",
    "        else:\n",
    "            input_text = ''.join(sentence_words)\n",
    "            input_encoded = tokenizer.encode(input_text)\n",
    "            with torch.no_grad():\n",
    "                result = model(torch.tensor(input_encoded).unsqueeze(0))[0]\n",
    "            result = result.squeeze(0)\n",
    "            \n",
    "            # Now some magic loop\n",
    "            index = 0\n",
    "            final_repr = list()\n",
    "            assert len(result) == len(input_encoded), \"Something is terribly wrong!\"\n",
    "            \n",
    "            print([z for z in sentence_words])\n",
    "            print([tokenizer.decode(z) for z in input_encoded])\n",
    "            #continue\n",
    "            for true in sentence_words:\n",
    "                \n",
    "                if len(result)>40: print(index, len(result), \"looking for\", true)\n",
    "                # If the words at the position match exactly, just append the representation\n",
    "                decoded_input_at_position = tokenizer.decode(input_encoded[index]).strip()\n",
    "                if decoded_input_at_position == true.strip():\n",
    "                    final_repr.append(result[index])\n",
    "                    index += 1\n",
    "                    \n",
    "                # If the words at the position do not match exactly, search for the end of the token\n",
    "                # Then average over all the representations in between\n",
    "                else:\n",
    "                    offset = 0\n",
    "                    for search in range(index, len(sentence_words)-1):\n",
    "                        offset += 1\n",
    "                        true_stripped = true.strip()\n",
    "                        current_search = tokenizer.decode(input_encoded[search]).strip()\n",
    "                        if true_stripped.endswith(current_search) or true_stripped in current_search:\n",
    "                            complete_substring = tokenizer.decode(input_encoded[index:search+1])\n",
    "                            to_append = torch.mean(result[index:search+1], dim=0)\n",
    "                            final_repr.append(to_append)\n",
    "                            break\n",
    "                    # Restart search at index + offset\n",
    "                    index += offset\n",
    "            # Done\n",
    "            final = torch.stack(final_repr)\n",
    "            sentences_result.append(final.squeeze(0))\n",
    "      \n",
    "    return sentences_result\n",
    "    \n",
    "assert_sen_reps(model, tokenizer, lstm, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH SENTENCE REPRESENTATIONS\n",
    "from torch import Tensor\n",
    "import pickle\n",
    "\n",
    "\n",
    "# I provide the following sanity check, that compares your representations against a pickled version of mine.\n",
    "# Note that I use the DistilGPT-2 LM here. For the LSTM I used 0-valued initial states.\n",
    "def assert_sen_reps(model, tokenizer, lstm, vocab):\n",
    "    with open('distilgpt2_emb1.pickle', 'rb') as f:\n",
    "        distilgpt2_emb1 = pickle.load(f)\n",
    "\n",
    "    with open('lstm_emb1.pickle', 'rb') as f:\n",
    "        lstm_emb1 = pickle.load(f)\n",
    "    \n",
    "    corpus = parse_corpus('data/sample/en_ewt-ud-train.conllu')\n",
    "    print(len(corpus))\n",
    "    corpus = corpus[20:]\n",
    "\n",
    "    own_distilgpt2_emb1 = fetch_sen_reps(corpus, model, tokenizer)[0]\n",
    "    own_lstm_emb1 = fetch_sen_reps(corpus, lstm, vocab)[0]\n",
    "    \n",
    "    assert distilgpt2_emb1.shape == own_distilgpt2_emb1.shape\n",
    "    assert lstm_emb1.shape == own_lstm_emb1.shape\n",
    "    \n",
    "    assert torch.allclose(distilgpt2_emb1,own_distilgpt2_emb1,atol=1e-05), \"GPT2 embeddings don't match!\"\n",
    "    assert torch.allclose(lstm_emb1,own_lstm_emb1,atol=1e-05), \"LSTM embeddings don't match!\"\n",
    "\n",
    "    print(\"All is well!\")\n",
    "    debug = False \n",
    "    \n",
    "    if debug:\n",
    "        for i in range(len(lstm_emb1)):\n",
    "            are_all_close = torch.allclose(lstm_emb1[i], own_lstm_emb1[i],atol=1e-05)\n",
    "            print(i, are_all_close, torch.max(lstm_emb1[i] - own_lstm_emb1[i]))\n",
    "            if not are_all_close:\n",
    "                print(\"LSTM embs don't match as position \" + str(i) + \" \" + corpus[0][i]['form'])\n",
    "\n",
    "        for i in range(len(lstm_emb1)):\n",
    "            are_all_close = torch.allclose(distilgpt2_emb1[i], own_distilgpt2_emb1[i] ,atol=1e-05)\n",
    "            print(i, are_all_close, torch.max(distilgpt2_emb1[i] - own_distilgpt2_emb1[i])) \n",
    "            if not are_all_close:\n",
    "                print(\"DistilGPT-2 embs don't match at position \" + str(i) + \" \" + corpus[0][i]['form'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FETCH POS LABELS\n",
    "\n",
    "\n",
    "# Should return a tensor of shape (num_tokens_in_corpus,)\n",
    "# Make sure that when fetching these pos tags for your train/dev/test corpora you share the label vocabulary.\n",
    "def fetch_pos_tags(ud_parses: List[TokenList], pos_vocab=None) -> Tensor:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Function that combines the previous functions, and creates 2 tensors for a .conllu file: \n",
    "# 1 containing the token representations, and 1 containing the (tokenized) pos_tags.\n",
    "\n",
    "def create_data(filename: str, lm, w2i, pos_vocab=None):\n",
    "    ud_parses = parse_corpus(filename)\n",
    "    \n",
    "    sen_reps = fetch_sen_reps(ud_parses, lm, w2i)\n",
    "    pos_tags, pos_vocab = fetch_pos_tags(ud_parses, pos_vocab=pos_vocab)\n",
    "    \n",
    "    return sen_reps, pos_tags, pos_vocab\n",
    "\n",
    "\n",
    "lm = model  # or `lstm`\n",
    "w2i = tokenizer  # or `vocab`\n",
    "use_sample = True\n",
    "\n",
    "train_x, train_y, train_vocab = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-train.conllu'),\n",
    "    lm, \n",
    "    w2i\n",
    ")\n",
    "\n",
    "dev_x, dev_y, _ = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-dev.conllu'),\n",
    "    lm, \n",
    "    w2i,\n",
    "    pos_vocab=train_vocab\n",
    ")\n",
    "\n",
    "test_x, test_y, _ = create_data(\n",
    "    os.path.join('data', 'sample' if use_sample else '', 'en_ewt-ud-test.conllu'),\n",
    "    lm,\n",
    "    w2i,\n",
    "    pos_vocab=train_vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diagnostic Classification\n",
    "\n",
    "We now have our models, our data, _and_ our representations all set! Hurray, well done. We can finally move onto the cool stuff, i.e. training the diagnostic classifiers (DCs).\n",
    "\n",
    "DCs are simple in their complexity on purpose. To read more about why this is the case you could already have a look at the \"Designing and Interpreting Probes with Control Tasks\" by Hewitt and Liang (esp. Sec. 3.2).\n",
    "\n",
    "A simple linear classifier will suffice for now, don't bother with adding fancy non-linearities to it.\n",
    "\n",
    "I am personally a fan of the `skorch` library, that provides `sklearn`-like functionalities for training `torch` models, but you are free to train your dc using whatever method you prefer.\n",
    "\n",
    "As this is an Artificial Intelligence master and you have all done ML1 + DL, I expect you to use your train/dev/test splits correctly ;-)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIAGNOSTIC CLASSIFIER\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trees\n",
    "\n",
    "For our gold labels, we need to recover the node distances from our parse tree. For this we will use the functionality provided by `ete3`, that allows us to compute that directly. I have provided code that transforms a `TokenTree` to a `Tree` in `ete3` format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you want to transform your conllu tree to an nltk.Tree, for better visualisation\n",
    "\n",
    "def rec_tokentree_to_nltk(tokentree):\n",
    "    token = tokentree.token[\"form\"]\n",
    "    tree_str = f\"({token} {' '.join(rec_tokentree_to_nltk(t) for t in tokentree.children)})\"\n",
    "\n",
    "    return tree_str\n",
    "\n",
    "\n",
    "def tokentree_to_nltk(tokentree):\n",
    "    from nltk import Tree as NLTKTree\n",
    "\n",
    "    tree_str = rec_tokentree_to_nltk(tokentree)\n",
    "\n",
    "    return NLTKTree.fromstring(tree_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install ete3\n",
    "from ete3 import Tree as EteTree\n",
    "\n",
    "\n",
    "class FancyTree(EteTree):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, format=1, **kwargs)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return self.get_ascii(show_internal=True)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return str(self)\n",
    "\n",
    "\n",
    "def rec_tokentree_to_ete(tokentree):\n",
    "    idx = str(tokentree.token[\"id\"])\n",
    "    children = tokentree.children\n",
    "    if children:\n",
    "        return f\"({','.join(rec_tokentree_to_ete(t) for t in children)}){idx}\"\n",
    "    else:\n",
    "        return idx\n",
    "    \n",
    "def tokentree_to_ete(tokentree):\n",
    "    newick_str = rec_tokentree_to_ete(tokentree)\n",
    "\n",
    "    return FancyTree(f\"{newick_str};\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check if it works!\n",
    "# We can read in a corpus using the code that was already provided, and convert it to an ete3 Tree.\n",
    "\n",
    "def parse_corpus(filename):\n",
    "    from conllu import parse_incr\n",
    "\n",
    "    data_file = open(filename, encoding=\"utf-8\")\n",
    "\n",
    "    ud_parses = list(parse_incr(data_file))\n",
    "    \n",
    "    return ud_parses\n",
    "\n",
    "corpus = parse_corpus('data/sample/en_ewt-ud-train.conllu')\n",
    "item = corpus[0]\n",
    "tokentree = item.to_tree()\n",
    "ete3_tree = tokentree_to_ete(tokentree)\n",
    "print(ete3_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see we label a token by its token id (converted to a string). Based on these id's we are going to retrieve the node distances.\n",
    "\n",
    "To create the true distances of a parse tree in our treebank, we are going to use the `.get_distance` method that is provided by `ete3`: http://etetoolkit.org/docs/latest/tutorial/tutorial_trees.html#working-with-branch-distances\n",
    "\n",
    "We will store all these distances in a `torch.Tensor`.\n",
    "\n",
    "Please fill in the gap in the following method. I recommend you to have a good look at Hewitt's blog post  about these node distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gold_distances(corpus):\n",
    "    all_distances = []\n",
    "\n",
    "    for item in (corpus):\n",
    "        tokentree = item.to_tree()\n",
    "        ete_tree = tokentree_to_ete(tokentree)\n",
    "\n",
    "        sen_len = len(ete_tree.search_nodes())\n",
    "        distances = torch.zeros((sen_len, sen_len))\n",
    "\n",
    "        # Your code for computing all the distances comes here.\n",
    "\n",
    "        all_distances.append(distances)\n",
    "\n",
    "    return all_distances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is now to do the previous step the other way around. After all, we are mainly interested in predicting the node distances of a sentence, in order to recreate the corresponding parse tree.\n",
    "\n",
    "Hewitt et al. reconstruct a parse tree based on a _minimum spanning tree_ (MST, https://en.wikipedia.org/wiki/Minimum_spanning_tree). Fortunately for us, we can simply import a method from `scipy` that retrieves this MST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "import torch\n",
    "\n",
    "\n",
    "def create_mst(distances):\n",
    "    distances = torch.triu(distances).detach().numpy()\n",
    "    \n",
    "    mst = minimum_spanning_tree(distances).toarray()\n",
    "    mst[mst>0] = 1.\n",
    "    \n",
    "    return mst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at what this looks like, by looking at a relatively short sentence in the sample corpus.\n",
    "\n",
    "If your addition to the `create_gold_distances` method has been correct, you should be able to run the following snippet. This then shows you the original parse tree, the distances between the nodes, and the MST that is retrieved from these distances. Can you spot the edges in the MST matrix that correspond to the edges in the parse tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item = corpus[5]\n",
    "tokentree = item.to_tree()\n",
    "ete3_tree = tokentree_to_ete(tokentree)\n",
    "print(ete3_tree, '\\n')\n",
    "\n",
    "gold_distance = create_gold_distances(corpus[5:6])[0]\n",
    "print(gold_distance, '\\n')\n",
    "\n",
    "mst = create_mst(gold_distance)\n",
    "print(mst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are able to map edge distances back to parse trees, we can create code for our quantitative evaluation. For this we will use the Undirected Unlabeled Attachment Score (UUAS), which is expressed as:\n",
    "\n",
    "$$\\frac{\\text{number of predicted edges that are an edge in the gold parse tree}}{\\text{number of edges in the gold parse tree}}$$\n",
    "\n",
    "To do this, we will need to obtain all the edges from our MST matrix. Note that, since we are using undirected trees, that an edge can be expressed in 2 ways: an edge between node $i$ and node $j$ is denoted by both `mst[i,j] = 1`, or `mst[j,i] = 1`.\n",
    "\n",
    "You will write code that computes the UUAS score for a matrix of predicted distances, and the corresponding gold distances. I recommend you to split this up into 2 methods: 1 that retrieves the edges that are present in an MST matrix, and one general method that computes the UUAS score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges(mst):\n",
    "    edges = set()\n",
    "\n",
    "    # Your code for retrieving the edges from the MST matrix\n",
    "\n",
    "    return edges\n",
    "\n",
    "def calc_uuas(pred_distances, gold_distances):\n",
    "    uuas = None\n",
    "    \n",
    "    # Your code for computing the UUAS score\n",
    "    \n",
    "    return uuas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structural Probes\n",
    "\n",
    "We now have everything in place to start doing the actual exciting stuff: training our structural probe!\n",
    "    \n",
    "To make life easier for you, we will simply take the `torch` code for this probe from John Hewitt's repository. This allows you to focus on the training regime from now on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "\n",
    "class StructuralProbe(nn.Module):\n",
    "    \"\"\" Computes squared L2 distance after projection by a matrix.\n",
    "    For a batch of sentences, computes all n^2 pairs of distances\n",
    "    for each sentence in the batch.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_dim, rank, device=\"cpu\"):\n",
    "        super().__init__()\n",
    "        self.probe_rank = rank\n",
    "        self.model_dim = model_dim\n",
    "        \n",
    "        self.proj = nn.Parameter(data = torch.zeros(self.model_dim, self.probe_rank))\n",
    "        \n",
    "        nn.init.uniform_(self.proj, -0.05, 0.05)\n",
    "        self.to(device)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        \"\"\" Computes all n^2 pairs of distances after projection\n",
    "        for each sentence in a batch.\n",
    "        Note that due to padding, some distances will be non-zero for pads.\n",
    "        Computes (B(h_i-h_j))^T(B(h_i-h_j)) for all i,j\n",
    "        Args:\n",
    "          batch: a batch of word representations of the shape\n",
    "            (batch_size, max_seq_len, representation_dim)\n",
    "        Returns:\n",
    "          A tensor of distances of shape (batch_size, max_seq_len, max_seq_len)\n",
    "        \"\"\"\n",
    "        transformed = torch.matmul(batch, self.proj)\n",
    "        \n",
    "        batchlen, seqlen, rank = transformed.size()\n",
    "        \n",
    "        transformed = transformed.unsqueeze(2)\n",
    "        transformed = transformed.expand(-1, -1, seqlen, -1)\n",
    "        transposed = transformed.transpose(1,2)\n",
    "        \n",
    "        diffs = transformed - transposed\n",
    "        \n",
    "        squared_diffs = diffs.pow(2)\n",
    "        squared_distances = torch.sum(squared_diffs, -1)\n",
    "\n",
    "        return squared_distances\n",
    "\n",
    "    \n",
    "class L1DistanceLoss(nn.Module):\n",
    "    \"\"\"Custom L1 loss for distance matrices.\"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, predictions, label_batch, length_batch):\n",
    "        \"\"\" Computes L1 loss on distance matrices.\n",
    "        Ignores all entries where label_batch=-1\n",
    "        Normalizes first within sentences (by dividing by the square of the sentence length)\n",
    "        and then across the batch.\n",
    "        Args:\n",
    "          predictions: A pytorch batch of predicted distances\n",
    "          label_batch: A pytorch batch of true distances\n",
    "          length_batch: A pytorch batch of sentence lengths\n",
    "        Returns:\n",
    "          A tuple of:\n",
    "            batch_loss: average loss in the batch\n",
    "            total_sents: number of sentences in the batch\n",
    "        \"\"\"\n",
    "        labels_1s = (label_batch != -1).float()\n",
    "        predictions_masked = predictions * labels_1s\n",
    "        labels_masked = label_batch * labels_1s\n",
    "        total_sents = torch.sum((length_batch != 0)).float()\n",
    "        squared_lengths = length_batch.pow(2).float()\n",
    "\n",
    "        if total_sents > 0:\n",
    "            loss_per_sent = torch.sum(torch.abs(predictions_masked - labels_masked), dim=(1,2))\n",
    "            normalized_loss_per_sent = loss_per_sent / squared_lengths\n",
    "            batch_loss = torch.sum(normalized_loss_per_sent) / total_sents\n",
    "        \n",
    "        else:\n",
    "            batch_loss = torch.tensor(0.0)\n",
    "        \n",
    "        return batch_loss, total_sents\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have provided a rough outline for the training regime that you can use. Note that the hyper parameters that I provide here only serve as an indication, but should be (briefly) explored by yourself.\n",
    "\n",
    "As can be seen in Hewitt's code above, there exists functionality in the probe to deal with batched input. It is up to you to use that: a (less efficient) method can still incorporate batches by doing multiple forward passes for a batch and computing the backward pass only once for the summed losses of all these forward passes. (_I know, this is not the way to go, but in the interest of time that is allowed ;-), the purpose of the assignment is writing a good paper after all_)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "'''\n",
    "Similar to the `create_data` method of the previous notebook, I recommend you to use a method \n",
    "that initialises all the data of a corpus. Note that for your embeddings you can use the \n",
    "`fetch_sen_reps` method again. However, for the POS probe you concatenated all these representations into \n",
    "1 big tensor of shape (num_tokens_in_corpus, model_dim). \n",
    "\n",
    "The StructuralProbe expects its input to contain all the representations of 1 sentence, so I recommend you\n",
    "to update your `fetch_sen_reps` method in a way that it is easy to retrieve all the representations that \n",
    "correspond to a single sentence.\n",
    "''' \n",
    "\n",
    "def init_corpus(path, concat=False, cutoff=None):\n",
    "    \"\"\" Initialises the data of a corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to corpus location\n",
    "    concat : bool, optional\n",
    "        Optional toggle to concatenate all the tensors\n",
    "        returned by `fetch_sen_reps`.\n",
    "    cutoff : int, optional\n",
    "        Optional integer to \"cutoff\" the data in the corpus.\n",
    "        This allows only a subset to be used, alleviating \n",
    "        memory usage.\n",
    "    \"\"\"\n",
    "    corpus = parse_corpus(path)[:cutoff]\n",
    "\n",
    "    embs = fetch_sen_reps(corpus, model, tokenizer, concat=concat)    \n",
    "    gold_distances = create_gold_distances(corpus)\n",
    "    \n",
    "    return gold_distances, embs\n",
    "\n",
    "\n",
    "# I recommend you to write a method that can evaluate the UUAS & loss score for the dev (& test) corpus.\n",
    "# Feel free to alter the signature of this method.\n",
    "def evaluate_probe(probe, _data):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    return loss_score, uuas_score\n",
    "\n",
    "\n",
    "# Feel free to alter the signature of this method.\n",
    "def train(_data):\n",
    "    emb_dim = 768\n",
    "    rank = 64\n",
    "    lr = 10e-4\n",
    "    batch_size = 24\n",
    "\n",
    "    probe = StructuralProbe(emb_dim, rank)\n",
    "    optimizer = optim.Adam(probe.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5,patience=1)\n",
    "    loss_function =  L1DistanceLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i in range(0, len(corpus), batch_size):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # YOUR CODE FOR DOING A PROBE FORWARD PASS\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        dev_loss, dev_uuas = evaluate_probe(probe, _dev_data)\n",
    "\n",
    "        # Using a scheduler is up to you, and might require some hyper param fine-tuning\n",
    "        scheduler.step(dev_loss)\n",
    "\n",
    "    test_loss, test_uuas = evaluate_probe(probe, _test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
